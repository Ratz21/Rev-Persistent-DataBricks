# Rev-Persistent-DataBricks

Real-Time Weather Streaming Pipeline (AWS + Databricks)


A complete end-to-end real-time data engineering pipeline built using AWS Kinesis, EC2, Firehose, S3, Databricks (Bronzeâ€“Silverâ€“Gold model), and Delta Lake.

This project demonstrates cloud-native ingestion, distributed ETL, Delta Lake storage optimization, aggregations, and dashboard-ready Gold tables.



                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   EC2 Weather Producer   â”‚
                  â”‚ (Python Random Generator)â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ JSON Events
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ AWS Kinesis Data â”‚
                       â”‚      Stream      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Kinesis Firehose â”‚
                       â”‚  (Buffer â†’ S3)   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  S3 Raw Storage  â”‚
                       â”‚  (Partitioned)   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚      Databricks Notebook    â”‚
                  â”‚    Bronze â†’ Silver â†’ Gold   â”‚
                  â”‚ Hive Metastore Delta Tables â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Gold Tables    â”‚
                       â”‚ (Hourly Metrics) â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Dashboard (SQL) â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸš€ Features Implemented
âœ… Real-time ingestion with AWS Kinesis Stream
âœ… Auto-delivery to S3 using Firehose with buffering + prefixing
âœ… EC2 Kinesis producer using boto3
âœ… Databricks ETL using Bronze â†’ Silver â†’ Gold
âœ… Delta Lake format for ACID reliability
âœ… Hive Metastore tables for easy SQL access
âœ… Visualization



ğŸ“‚ Project Structure
weather-streaming-pipeline/
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ producer_kinesis.py         # Python script sending weather events
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ 01_bronze_ingestion.py      # Bronze: raw ingest + standardization
â”‚   â”œâ”€â”€ 02_silver_cleaning.py       # Silver: cleansing + typing
â”‚   â”œâ”€â”€ 03_gold_aggregation.py      # Gold: hourly weather metrics
â”‚   â””â”€â”€ dashboard_sql.txt           # Queries used for dashboard
â”‚
â”œâ”€â”€ README.md                        # Main documentation (this file)
â””â”€â”€ architecture.png                 # Optional diagram



1ï¸âƒ£ Data Ingestion Layer (AWS)
ğŸ–¥ï¸ EC2 Weather Producer

A Python script runs on EC2 and streams JSON weather events to Kinesis Data Stream every second.

Example event:

{
  "station_id": "ST_3",
  "timestamp": "2025-12-08T17:22:11Z",
  "temperature_c": 29.4,
  "humidity": 62.5,
  "wind_speed": 11.3
}

ğŸ”¥ Kinesis â†’ Firehose â†’ S3
Firehose settings used
Setting	Value
Source	Kinesis Stream
Buffer Size	1 MiB
Buffer Interval	60 sec
S3 Prefix	kinesis/weather/!{timestamp:yyyy}/!{timestamp:MM}/!{timestamp:dd}/!{timestamp:HH}/
Compression	GZIP
Format	JSON

This creates S3 folders like:

s3://weather-stream-raw-ap-south-1/kinesis/weather/2025/12/08/17/

2ï¸âƒ£ Bronze Layer â€“ Raw Data Standardization
Why Bronze?

Ingest raw JSON exactly as produced

Add schema, metadata, and recursive ingestion

Store as Delta for ACID and schema evolution

Bronze code:
raw_path = "s3a://weather-stream-raw-ap-south-1/kinesis/weather/*/*/*/*/"

raw_df = (
    spark.read
         .option("recursiveFileLookup", "true")
         .json(raw_path)
)

bronze_df = raw_df.withColumn("ingest_time", current_timestamp())

bronze_df.write.format("delta").mode("overwrite").save(bronze_path)

Bronze Table
hive_metastore.weather_db.weather_bronze

3ï¸âƒ£ Silver Layer â€“ Cleaning & Transformations
Why Silver?

Fix corrupted or missing values

Convert datatypes

Filter out invalid rows

Standardize timestamp fields

Sample Transformations
silver_df = bronze_df.select(
    col("station_id"),
    to_timestamp("timestamp").alias("event_time"),
    col("temperature_c").cast("double"),
    col("humidity").cast("double"),
    col("wind_speed").cast("double"),
    col("ingest_time")
).dropna()

Silver Table
hive_metastore.weather_db.weather_silver

4ï¸âƒ£ Gold Layer â€“ Analytics Aggregation
Why Gold?

Business-ready metrics

Aggregations for dashboards

Low-latency reporting tables

Gold performs hourly aggregation per station:

gold_df = (
    silver_df
    .groupBy(
        window("event_time", "1 hour"),
        col("station_id")
    )
    .agg(
        avg("temperature_c").alias("avg_temp_c"),
        min("temperature_c").alias("min_temp_c"),
        max("temperature_c").alias("max_temp_c"),
        avg("humidity").alias("avg_humidity"),
        avg("wind_speed").alias("avg_wind_speed"),
        count("*").alias("row_count")
    )
)

Gold Table
hive_metastore.weather_db.weather_gold_hourly

5ï¸âƒ£ Dashboard Layer (Databricks SQL)

Example SQL for visualization:

SELECT 
  station_id,
  window_start,
  avg_temp_c,
  avg_humidity,
  avg_wind_speed
FROM weather_db.weather_gold_hourly
ORDER BY window_start DESC;


Charts you can build:

Temperature trend (line chart)

Humidity trend

Station-wise metrics (bar chart)

Wind speed distribution

ğŸ§  Why Hive Metastore Instead of Unity Catalog?

Your workspace did not have Unity Catalog enabled
(which requires account-level setup + storage credentials).

For a single-team POC, hive_metastore is sufficient.

ğŸ§± Why Delta Lake?
Feature	Benefit
ACID Transactions	No partial writes, safe pipeline reruns
Schema Enforcement	Prevent bad JSON records
Time Travel	Debug & rollback
High-performance reads	Used in dashboard queries
Auto-optimized files	Faster aggregations
ğŸ“Š Business Value Delivered

Real-time weather monitoring

Auto-processing of continuous stream

Clean, consistent analytical dataset

Dashboard-ready aggregated metrics

Demonstrates complete data engineering lifecycle

âš™ï¸ Technologies Used
Component	Technology
Streaming	AWS Kinesis
ETL	Databricks
Storage	S3 + Delta Lake
Metadata	Hive Metastore
Compute	Databricks Cluster
Visualization	Databricks SQL Dashboard
Language	Python + PySpark
ğŸ“˜ Future Enhancements

Migrate tables to Unity Catalog

Add DLT (Delta Live Tables) for managed pipelines

Add Slack Notifications on job failures

Deploy producer using Docker + ECS

Add ML model monitoring weather anomalies

ğŸ™Œ Acknowledgments

This project was built for hands-on understanding of cloud data engineering and real-time analytics.









                       
